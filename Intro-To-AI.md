27-SEP-2022: THIS IS A WORK IN PROGRESS 

# Artificial Intelligence for Wildlife Conservation 

<p align="center">
  <img src="https://www.ft.com/__origami/service/image/v2/images/raw/http://prod-upp-image-read.ft.com/a6fcca88-112a-11ea-a7e6-62bf4f9e548a?source=next&fit=scale-down&quality=highest&width=1440" width="900"/>
</p>

Image credit: Ian Bott 

## Tech is transforming wildlife conservation 

(Perspectives in machine learning for wildlife conservation) 
Inexpensive and accessible sensors are accelerating data acquisition in animal ecology. These technologies hold great potential for large-scale ecological understanding, but are limited by current processing approaches which inefficiently distill data into relevant information. We argue that animal ecologists can capitalize on large datasets generated by modern sensors by combining machine learning approaches with domain knowledge. Incorporating machine learning into ecological workflows could improve inputs for ecological models and lead to integrated hybrid modeling tools. This approach will require close interdisciplinary collaboration to ensure the quality of novel approaches and train a new generation of data scientists in ecology and conservation.

How are animals currently monitored? Conventionally, management and conservation of animal species are based on data
collection carried out by human field workers who count animals,
observe their behavior, and/or patrol natural reserves. Such efforts
are time-consuming, labor-intensive, and expensive3. They can
also result in biased datasets due to challenges in controlling for
observer subjectivity and assuring high inter-observer reliability,
and often unavoidable responses of animals to observer
presence4,5. Human presence in the field also poses risks to
wildlife6,7, their habitats8, and humans themselves: as an example,
many wildlife and conservation operations are performed from
aircraft and plane crashes are the primary cause of mortality for
wildlife biologists9. Finally, the physical and cognitive limitations
of humans unavoidably constrain the number of individual animals that can be observed simultaneously, the temporal resolution and complexity of data that can be collected, and the extent
of physical area that can be effectively monitored10,11


ecological sensing data -- 

Goal: understand animal occurrence, abundance, distribution, behavior and (mitigate) THREATS to animals and ecosystems (e.g., logging) 
In the ongoing quest to better understand and conserve wildlife populations, technology-enabled sampling methods have become increasingly important (Zemanova, 2020).
For example, annual ecological publications using camera traps – remote cameras that are triggered to take images of passing wildlife -- have grown 81-fold since 1994 (Delisle et al. 2021)
Other types of tech-enabled sampling methods include passive acoustic monitors, wildlife collars, drone or aerial imagery
![image](https://user-images.githubusercontent.com/6107689/192573639-c80b0388-55ad-4dec-802b-c4d9e535c916.png)


### Sensors

New sensors expand available data types for animal ecology. Sensor data provide a variety of perspectives to observe wildlife, monitor populations, and understand behavior. They allow the field to scale studies in space, time, and across the taxonomic tree and, thanks to open science projects (Table 2), to share data across parks, geographies, and the globe51. Sensors generate diverse data types, including imagery, soundscapes, and positional data (Fig. 3). They can be mobile or static, and can be deployed to collect information on individuals or species of interest (e.g., bio-loggers, drones), monitor activity in a particular location (e.g., camera traps and acoustic sensors), or document changes in habitats or landscapes over time (satellites, drones). Finally, they can also be opportunistic, as in the case of community science.

Recent advances in sensor technologies are drastically
increasing data collection capacity by reducing costs and
expanding coverage relative to conventional methods (see the
section “New sensors expand available data types for animal
ecology”, below), thereby opening new avenues for ecological
studies at scale (Fig. 1)18. Many previously inaccessible areas of
conservation interest can now be studied through the use of highresolution remote sensing19, and large amounts of data are being
collected non-invasively by digital devices such as camera traps20,
consumer cameras21, and acoustic sensors22. New on-animal biologgers, including miniaturized tracking tags23,24 and sensor
arrays featuring accelerometers, audiologgers, cameras, and other
monitoring devices document the movement and behavior of
animals in unprecedented detail25, enabling researchers to track
individuals across hemispheres and over their entire lifetimes at
high temporal resolution and thereby revolutionizing the study of
animal movement (Fig. 1c) and migrations.


<p align="center">
  <img src="https://i.imgur.com/w4FFuPD.png" width="600"/>
</p>

Image credit: ["Perspectives in machine learning for wildlife conservation"](https://www.nature.com/articles/s41467-022-27980-y)

In this course, we'll be focusing on wildlife data collected from two sources: **camera traps** and **bioacoustic sensors**. 

***Camera traps*** are rugged cameras, deployed in the field, which are automatically triggered by passing animals, unobtrusively collecting data on medium- and large-bodied vertebrates. Camera traps are inexpensive, easy to use, and can provide a wide variety of information (e.g., species, sex, heath, age, behavior, ecological interactions) on entire ecology communities that can be used to assess occurrence, richness, distribution, density, and species interactions. More than a [million camera traps](https://www.nature.com/articles/s41467-022-27980-y) are currently used to monitor biodiversity worldwide. An average-sized camera trap survey ([~78 cameras](https://esajournals.onlinelibrary.wiley.com/doi/10.1002/fee.1448)) can capture millions of photographs every year. 

When a camera trap is triggered, a sequence of images (or a video) is taken. Every couple of weeks or months, researchers return to the cameras to download the camera memroy cards. Images are then manually sorted through by experts to classify species and remove empty images. While cameras enable the automatic collection of massive quantities of ecological data, the enormous amount of time required to sort through the images severely limits their application in conservation or management practice and reduces research productivity.

- To learn more about camera trapping, check out WWF's comprehensive [Guide to Camera Trapping](https://www.wwf.org.uk/sites/default/files/2019-04/CameraTraps-WWF-guidelines.pdf)

***Bioacoustic sensors*** (also known as ***acoustic monitors***) are similar to camera traps, except that they capture sound instead of images. These sensors are microphones or hydrophones (underwater microphones) that record animal vocalizations, which can be analyzed to detect what species are active in a given area, when they are active, and in some cases, what they are doing. Networks of sensors can be used to derive the distribution, occupancy, density, and richness of vocalizing species. Inexpensive, open-source accoustic sensors such as the [AudioMoth](https://www.openacousticdevices.info/audiomoth) are making these types of studies increasingly popular. 

Unlike camera traps, acoustic monitors are typically not triggered, but run continuously during pre-defined time periods (24/7, between the hours of dawn and dusk, for 30 minutes every hour, etc.). Researchers must then review the audio files to identify and classify calls or noises of interest. Hours, days, or weeks of audio can be collected during a single field deployment; multipled by tjhe number of acoustic sensors in your network, and this data becomes overwhelming. The substantial delay between data collection and when critical sounds are identified (i.e., gunshots of poachers), reduces their usefulness for conservation applications. 

- WWF also has a great [Guide to Acoustic Monitoring](https://www.wwf.org.uk/sites/default/files/2019-04/Acousticmonitoring-WWF-guidelines.pdf)  

## Harnessing the power of "Big Data" 

<p align="center">
  <img src="https://basis.net/wp-content/uploads/2019/02/Intro-to-AI.jpg" width="600"/>
</p>

Image credit: Stefanie Hoffman 

> "There is a mismatch between the ever-growing volume of raw measures (videos, images, audio recordings) acquired for ecological studies and our ability to process and analyze this multi-source data to derive conclusive ecological insights rapidly and at scale. Effectively, ecology has entered the age of **big data**." (Tulia et al. 2022)

What is Big Data? 



**camera traps** Once collected by cameras, each image must be reviewed and classified by species, and may be further classified by characteristics of the individual(s) photographed (e.g. age, sex, and behavior). With many mid-to-large scale projects amassing millions of photos and reaching terabytes of data in less than a year, the time committed to processing these data becomes increasingly unmanageable, ballooning time and monetary budgets.
![image](https://user-images.githubusercontent.com/6107689/192573924-7c5d556e-8886-46ce-82dd-f99ca2b7c17d.png)

Camera traps are a tool used by conservationists to study and monitor a wide range of ecologies while limiting human interference. However, they also generate a vast amount of data that quickly exceeds the capacity of humans to sift through. That's where machine learning can help! Advances in computer vision can help automate tasks like species detection and identification, so that humans can spend more time learning from and protecting these ecologies![image](https://user-images.githubusercontent.com/6107689/192574131-94201a55-3a5a-4c3c-992c-f1e098db0e6a.png)

However, with the number of camera trap images quickly outgrowing the capacity of the labelers, ecologists are unable to keep up with the wealth of data they are obtaining. Using computer vision, we can automatically generate labels for new camera trap images at the rate that they are being obtained, allowing ecologists to uncover ecological and biological information at a scale previously not possible. 
![image](https://user-images.githubusercontent.com/6107689/192574619-993699bb-9d71-425b-91b5-50bb04e67585.png)


There is a mismatch between the ever-growing volume of raw
measures (videos, images, audio recordings) acquired for ecological
studies and our ability to process and analyze this multi-source
data to derive conclusive ecological insights rapidly and at scale.
Effectively, ecology has entered the age of big data and is
increasingly reliant on sensors, advanced methodologies, and
computational resources26. Central challenges to efficient data
analysis are the sheer volume of data generated by modern collection methods and the heterogeneous nature of many ecological
datasets, which preclude the use of simple automated analysis
techniques26

The sensor data described in the previous section has the
potential to unlock ecological understanding on a scale difficult to
imagine in the recent past. But to do so, it must be interpreted
and converted to usable information for ecological research. 


Lots of data but it's not being used for conservation – why?
Too much data (manual data processing doesn’t scale) 
For example, one two-year study resulting 2.6 million images from 98,189 detections (McShea et al., 2016) across six states in the eastern USA.
![image](https://user-images.githubusercontent.com/6107689/192573739-4d900470-9b21-431c-984e-382efd9ece4b.png)

image from sara beery



owever, problems related to lack of generality across geographies, day/night acquisition, or sensors are still major obstacles to production-ready accurate systems55. The increased scale of available data due to de-siloing efforts from organizations like Wildlife Insights (www.wildlifeinsights.org) and LILA.science (www.lila.science) will help increase ML accuracy and robustness across regions and taxa. 


 Handling and analyzing these datasets efficiently requires access to advanced computing infrastructure and solutions. Second, the inherent complexity of soundscapes requires noise-robust algorithms that generalize well and can separate and identify many animal sounds of interest from confounding natural and anthropogenic signals in a wide variety of acoustic environments62. The third challenge is the lack of large and diverse labeled datasets. As for camera trap images, species- or regionspecific characteristics (e.g., regional dialects63) affect algorithm performance. Robust, large-scale datasets have begun to be curated for some animal groups (e.g., www.macaulaylibrary.org and www.xeno-canto.org for birds), but for many animal groups as well as relevant biological and non-biological confounding signals, such data is still nonexistent.
 
 ------- 


(https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13256)
Machine learning in general refers to a category of algorithms that can automatically generate predictive models by detecting patterns in data. These tools are interesting for ecologists because they can analyse complex nonlinear data, with interactions and missing data, which are frequently encountered in ecology (Olden et al., 2008). Machine learning has already been successfully applied in ecology to perform tasks such as classification (Cutler et al., 2007), ecological modelling (Recknagel, 2001) or studying animal behaviour (Valletta, Torney, Kings, Thornton, & Madden, 2017). 

What makes deep learning algorithms different and so powerful resides in the way they can learn features from data. First, learning can occur without supervision, where computers automatically discover patterns and similarities in unlabelled data. With this method, no specific output is expected, and this is often used as an exploratory tool to detect features in data, reduce its dimensions, or cluster groups of similar data (Valletta et al., 2017). Second, learning can also be done with supervised training. A labelled dataset with the target objects is first given to the computers so they can train to associate the labels to the examples. They can then recognize and identify these objects in other datasets (LeCun, Bengio, & Hinton, 2015). However, in conventional machine learning, providing only the labels is insufficient. The user also needs to specify in the algorithm what to look for (Olden et al., 2008). For instance to detect giraffes in images, the algorithm requires specific properties of giraffes (e.g. shape, colour, size, patterning) to be explicitly stated in terms of patterns of pixels. This can hamper non-specialists of machine learning because it usually requires a deep knowledge of the studied system and good programming skills. In contrast, deep learning methods skip such a step. Using general learning procedures, deep learning algorithms are able to automatically detect and extract features from data. This means that we only need to tell a deep learning algorithm whether a giraffe is present in a picture and, given enough examples, it will be able to figure out by itself what a giraffe looks like. Such an automated learning procedure is made possible by decomposing the data into multiple layers, each with different levels of abstraction, that allow the algorithm to learn complex features representing the data.

From a technical standpoint, deep learning algorithms are multilayered neural networks. Neural networks are models that process information in a way inspired by biological processes, with highly interconnected processing units called neurons working together to solve problems (Olden et al., 2008) (Figure 1). Neural networks have three main parts: (a) an input layer that receives the data, (b) an output layer that gives the result of the model, and (c) the processing core that contains one or more hidden layers. What differentiates a conventional neural network from a deep one is the number of hidden layers, which represents the depth of the network. Unfortunately, there is no consensus on how many hidden layers are required to differentiate a shallow from a deep neural network (Schmidhuber, 2015).

Two of the most common questions encountered is why use deep learning instead of ‘traditional’ machine learning and how is it different. The main difference with other methods lies in the way features are extracted from data. With traditional machine learning algorithms, feature extraction requires human supervision, whereas deep learning tools can learn by themselves very complex representations of data due to their multilayered nature. They are therefore easier to use when the users have limited knowledge about the features to detect. The record-breaking accuracy results achieved in identification and classification tasks (e.g. Krizhevsky et al., 2012; Joly et al., 2018) also leads to one of the main reasons to use deep learning: performance. However, these results depend on the existence of a sizeable labelled dataset that can be used to train the algorithms to extract the desired features from the data. The training process can be more time consuming and require a lot more computer power than traditional methods. Deep learning is thus especially appropriate when analysing large amounts of data, and it performs particularly well for complex tasks such as image classification or speech/sound recognition.


Machine learning
The rising availability of computers around the 1980s allowed not only more refined
numerical solutions for classical statistical methods, but also the development of alternative
modelling approaches for data analysis and predictions that we collectively refer to as
“machine learning”. Although these approaches differ in detail, we see their communality in
the realization that abandoning the data-generating model (connected to the ability to
calculate p-values, CIs and all that) in favor of generic algorithmic structures that can be
trained to data often achieves lower errors for predictive tasks (for general ML principles, see
Box 1) (Breiman, 2001b; Shmueli, 2010). Examples of early ML algorithms are neural networks
(McCulloch & Pitts, 1943), random forest (Breiman, 2001a), and boosted regression trees
(Friedman, 2001) (more on these in the section ‘Important ML Algorithms in more detail’).

General objective of ML
The objective of ML is to build a good predictive model. By “good”, we mean that the model should predict
well to new data. Sometimes ML models make almost no errors on the data they are trained on but fail for
new data (we say the model overfits). A more complex and flexible model has a higher risk of overfitting. The
trade-off between complexity and flexibility can be depicted by the bias-variance tradeoff (see Fig. 3a). The
general ideal of ML algorithms is thus to take a certain algorithmic structure and then adjust their parameters
to the data (training), while simultaneously adapt its complexity by optimizing the bias-variance trade-off so
that the fitted model generalizes well to new data.
Training the models
Training a model consists of two steps. The first step is the definition of a loss function, which measures the
current score (performance) of the algorithm in solving a certain task. The second component is the optimizer,
which updates parameters of the algorithm with the goal to increase its performance.
Model classes and architectures
In principle, any algorithm that makes predictions for a certain task can be used for ML. In practice, model
classes and architectures that are commonly used can broadly be divided into neural networks, which mimic
the functioning of a brain, regression and classification trees, and distance-based method.
Tasks and learning situations
In ML, the different use cases for the algorithms are called tasks. In supervised learning, there are examples
for the “correct” prediction of the task, and the objective functions report the error between the predicted
and observed response. Here, the common tasks are classification (e.g. labeling of images) and regression
(prediction of a numeric variable). These tasks are performed in different learning situations. Unsupervised
learning are tasks where the response are unknown (e.g. genomic species delimitation, see Derkarabetian et
al., 2019). Finally, in Reinforcement learning, the ML algorithm is trained by interacting with a (virtual)
environment. Reinforcement learning is used in tasks in which the learning depends on executed actions and
their produced consequences, for instance, playing strategy computer games such as DOTA (OpenAI et al.,
2019) or Starcraft (Vinyals et al., 2019).

Deep learning
The co-evolution of computational resources and ML algorithms experienced a final peak with
the emergence of DL algorithms in the last decade. DL algorithms are neural networks
(McCulloch & Pitts, 1943) that differ from classical artificial neural networks mainly by their
size. While many algorithms and network architectures that are used today were already
described in the ‘80s and ‘90s (e.g. Fukushima, 1980; Lecun et al., 1998), their practical
application was prevented by the lack of computing power at the time. This changed with the
emergence of graphical processing units (GPU) in the ‘90s (Fig. 1). Although GPUs were
originally developed for computer games or other graphical rendering tasks, it was quickly
realized that they are often far more efficient than CPUs for certain numeric and linear algebra
tasks. Krizhevsky et al., 2012 ushered in the new era of DL when they demonstrated that their
competition-winning neural network could be trained within hours on a GPU instead of days
or weeks using a CPU. Today, large DL models trained on GPUs with hundreds of millions
parameters dominate the competition for many complex ML tasks, and their behavior often
differs markedly from simple ML algorithms (see section “Why ML works”)

Deep learning
DL models represent the latest methodological advance in machine. DL algorithms are neural
networks, with the difference that they include a large number of hidden layers (Borowiec et
al., 2022; LeCun et al., 2015) and that the arrangement of the hidden layers (= architecture)
is often different from the simple, fully connected ANN. Complex task-specific architectures,
often with millions of parameters and specific structures, evolved over the years(for example
residual neural networks (He et al., 2016), see also Table 1).
Neural networks
Possibly the most iconic ML architecture is a neural network, which is inspired by the
architecture of our brains. The first fully functioning artificial neural network (ANN) was
described by Rosenblatt, 1958. This “perceptron algorithm” was a binary classifier that
connected the input neurons (one for each input variable = feature) to an output neuron
(response). If the signal in the output crossed a certain threshold (activation function), the
predicted class changed (e.g. from ‘0’ to ‘1’). However, because of its limited flexibility, and
particularly its inability to represent nonlinear relationships, the perceptron fell into oblivion
for many years until it was discovered that additional layers between the input and outout
neurons (so-called ‘hidden’ layers) made it possible to fit any type of function shape (see
subsection “deep learning”). The added flexibility is achieved by the hidden layers in
conjunction with the non-linear activation function: as in the brain, a hidden neuron will only
‘fire’ if the accumulated signals of the previous layers surpass a certain threshold. This
structure allows ANNs to flexibly approximate practically any functional shape. The potential
of ANNs for ecological applications was early recognized (e.g. Foody, 1995; Simpson et al.,
1992; Tilman, 1977), although to today they have largely been replaced by the more advanced
Deep Neural Networks in E&E (see section “Deep Learning”).




There are several ways to describe what artificial neural networks are and how they are used as inference tools. Although the most obvious biological analogy is limited, it is helpful to visualize neural networks as computer algorithms inspired by the brain: composed of interconnected layers of nodes (‘neurons’) and connections (‘synapses’) capable of learning by changing how easy it is for neurons to fire and how strong the connections are (Box 1). Computers represent these layers and connections as matrices of numbers manipulated through linear algebra operations, ensuring that neural networks can be used with virtually any input that can be represented numerically (Figure 1). In a mathematical sense, neural networks are simply a function mapping input onto a desired output. 



Machine learning (ML, see glossary in Supplementary Table 1)
deals with learning patterns from data28. Presented with large
quantities of inputs (e.g., images) and corresponding expected
outcomes, or labels (e.g., the species depicted in each image), a
supervised ML algorithm learns a mathematical function leading
to the correct outcome prediction when confronted with new,
unseen inputs. When the expected outcomes are absent, the (this
time unsupervised) ML algorithm will use solely the inputs to
extract groups of data points corresponding to typical patterns in
the data. ML has emerged as a promising means of connecting the
dots between big data and actionable ecological insights29 and is
an increasingly popular approach in ecology30,31. A significant
share of this success can be attributed to deep learning (DL32), a
family of highly versatile ML models based on artificial neural
networks that have shown superior performance across the
majority of ML use cases (see Table 1 and Supplementary
Table 2). Significant error reduction of ML and DL with respect to
traditional generalized regression models has been reported routinely for species richness and diversity estimation33,34. Likewise,
detection and counting pipelines moved from rough rule of thumb
extrapolations from visual counts in national parks to ML-based
methods with high detection rates. Initially, these methods proposed many false positives which required further human
review35, but recent methods have been shown to maintain high
detection rates with significantly fewer false positives36. As an
example, large mammal detection in the Kuzikus reserve in 2014
was improved significantly by improving the detection methodologies, from a recall rate of 20%35 to 80%37 (for a common
75% precision rate). Finally, studies involving human operators
demonstrated that ML enabled massive speedups in complex tasks
such as individual and species recognition38,39 and large-scale
tasks such as animal detection in drone surveys40. Recent advances
in ML methodology could accelerate and enhance various stages
of the traditional ecological research pipeline (see Fig. 2), from
targeted data acquisition to image retrieval and semi-automated
population surveys.



Deep learning, a subclass of machine learning, uses artificial neural networks to process information (Lamba et al., 2019). Artificial neural networks are based on the neural layout seen in biological systems, allowing computer-based algorithms to “learn” based on training data, in order to accurately process similar yet distinct data at a later time. A rapidly expanding subfield of this technology is computer vision, where a multilayered model is trained on a large number of previously classified images, and then applied to new images in order to assign classifications without human interaction (Weinstein, 2018).![image](https://user-images.githubusercontent.com/6107689/192487662-2144ed6f-c8d7-4bad-a5db-af903b7d0e56.png)

Training, learning, models, recognition and confidence.
Image recognition systems must be trained via machine learning, where it learns how to distinguish the
contents of one image from another. Training and thus learning begin with a large set of previously
labelled (i.e., already categorized) images. The system analyzes those images and its labels to create a
model (technically called a ‘convolution neural network’) that best fits what it sees. To achieve
recognition, the system tries to best match an unlabelled (i.e., previously unseen) image to that model,
where its predictions are those classifications that match what is in the model. The model is 
sophisticated, where it can associate a confidence with that prediction (usually a number between 0 and
1). However, confidence should be used only as a very rough indication of likely correctness. Interpret a
high confidence value as ‘likely correct with occasional errors’ and a low confidence value as ‘likely
incorrect and a large number of errors’.
Of course, image recognition is more complex than that. The key take-away is that training is critical.
Good training requires a very large number of correctly labelled images, which in turn require many
varied images per location and desired classification. 
Saul Greenberg (https://prism.ucalgary.ca/bitstream/handle/1880/112416/2020-08-Greenberg-ImageRecognitionCameraTraps.pdf?sequence=6) 

 illustrate each with an example that assumes that only images containing deer are requested from the
image recognizer.
• Precision:
o What proportion of the classification results to contain an entity are actually correct?
o Formula: true positives retrieved / all true and false positives retrieved
o Of the 20 deer classifications returned only 16 of them are actually deer (the remainder are
incorrectly classified). The precision is 16/20 or 80%.
• Recall
o What is the proportion of the positive classifications results returned vs the total true
positives available in the set?
o Formula: true positives retrieved / all true positives and false negatives in the set
o While 16 correct deer classifications are returned, a total of 24 deer are actually present
across the images. The recall is 16/24, or 66%.
• Accuracy
o What is the proportion of returned results that are correct (either positive or negative)
o Formula: (True positives + true negatives) / (true positives + true negatives + false positives +
false negatives)
• F-Score
o Combines the precision and recall measures into an approximate average.
o Formula: 2 * precision * recall / (precision + recall)
For the examples above, this is 2 * .8 * .66 / (.8 + .66), or 72%.



file:///Users/meredithpalmer/Desktop/Seeing_biodiversity_perspectives_in_machine_learni.pdf
Data acquisition in animal ecology is rapidly accelerating due to
inexpensive and accessible sensors such as smartphones, drones,
satellites, audio recorders and bio-logging devices. These new
technologies and the data they generate hold great potential for
large-scale environmental monitoring and understanding, but
are limited by current data processing approaches which are
inefficient in how they ingest, digest, and distill data into relevant information. We argue that machine learning, and especially deep learning approaches, can meet this analytic challenge to enhance our understanding, monitoring capacity, and
conservation of wildlife species. Incorporating machine learning into ecological workflows could improve inputs for population and behavior models and eventually lead to integrated
hybrid modeling tools, with ecological models acting as constraints for machine learning models and the latter providing
data-supported insights. In essence, by combining new machine learning approaches with ecological domain knowledge,
animal ecologists can capitalize on the abundance of data generated by modern sensor technologies in order to reliably estimate population abundances, study animal behavior and mitigate human/wildlife conflicts.


 
 
 
## What is artificial intelligence?

So what exactly is **artificial intelligence (AI)**? AI is intelligent software-based technology that receives information from the environment and then takes actions based on that information that affect the environment. At it's core, AI is the ability for machines to simulate and enhance human intelligence. 

A subset of AI is **machine learning (ML)**. Machine learning algorithms are a collection of mathematical and statistical models that learn representations from the underlying training data. Basically, machine learning algorithms extract patterns from input data in order to come up with the rules and the parameters of these models, so that they can make smart predictions and decisions. 

A subset of machine learning is **deep learning**. Deep learning models are basically very complex machine learning models. They learn representations successively in layers.

We'll be working primarily with deep learning models today, but may use these terms interchangeably. 

<p align="center">
  <img src="https://i.imgur.com/TNDF3fR.png" width="600"/>
</p>

Image credit: [McClure et al. 2020](https://www.sciencedirect.com/science/article/pii/S2666389920301434)

### Key terms

There are a few important definitions to keep in mind as we go through this workshop: 

-	**Algorithm**: A sequence of instructions to a computer, like a recipe on how to process information
- **Model**: An artifact you get after feeding data into a machine learning algorithm (feed inputs, get output)
- **Inference**: The process of running inference is feeding the input data through the model to get output (i.e., some kind of prediction) 

## How machine learning works 

How do machine learning algorithms differ from "traditional" data science algorithms? 

[Alexander Fred-Ojala](XXX) explains... 

>Traditional algorithms rely on parameters and rules defined by humans; data are then processed according to these rules to produce results.

<p align="center">
  <img src="https://i.imgur.com/07d7iPV.png" width="300"/>
</p>

>Machine learning algorithms, on the other hand, extract rules from data; parameter values and rules are decided during the training process (described in WHERE). In order to train a supervised machine learning algorithm, you only have to provide it with answers together with input data and out would come the rules. These rules can then be used in order to predict answers and outputs on data that it hasn't been trained on before. 

<p align="center">
  <img src="https://i.imgur.com/Es57Hm3.png" width="370"/>
</p>

### When is machine learning useful? 

Before you begin, it is worth thinking aout what can and cannot be addressed using machine learning. Machine learning is not always the right tool for the you! Machine learning is a good approach when you have: 

- Lots of clean, labelled data
- A tightly scoped problem 
- Some margin of error is okay 

Maybe consider an alternative option to machine learning if: 
- Conditions vary beyond original dataset
- There is no data readily available
- There is no tolerability of error 

### Type of data you can use 

Many kinds of data can be fed into machine learning algorithsm. For those thinking about using AI for wildlife ecology and conservation applications, you 

TAKE SCREENSHOT FROM DAN'S TALK 
-images, audio, time series sensor data (vibration, temp, aceerlation), positional data (GPS), combos of these

PUT SOME IMAGES HERE 


## Types of machine learning 

There are three main categories of machine learning algorithms. 

The first two relate to **supervised machine learning**. Supervised machine learning is all about trying to find a function that can map some input data to some output that could be a prediction or a classification. In supervised machine learning, you need to provide data during the training of these machine learning algorithms that are correct input and output pairs. You need to have the true outputs or the true labels associated with your training data in order for you to train these algorithms. Supervised machine learning algorithms could be regression models. They predict a continuous outcome variable, or it could be classification algorithms and models, and they predict or classify a certain set of categories or labels. 

We also have **unsupervised machine learning**. For unsupervised machine learning models, we don't provide any correct labels or outputs on the training data. Instead, we try to extract and parse patterns so that we, for example, could carry out clustering, dimensionality reduction, outlier detection, segmentation, et cetera.

<p align="center">
  <img src="https://i.imgur.com/jb4UVfX.jpg" width="600"/>
</p>

Image credit: Alexander Fred-Ojala

Today, we'll focus on the 'classification' aspect of supervised machine learning and how we can apply that to tackle ecological and conservation data. 

## Machine learning workflow:  

1. Formulate your question in terms of what is observed and what answer you want the model to predict
2. Obtain a clean, representative dataset for training 
3. Train your model (an interative process) 
4. You have a model! (or you gave up)
5. Monitor your model periodically to make sure it is working 

Below, we'll dive deeper into putting together a dataset and training your model: 

### Obtain a clean, representative dataset 

***Labeled data***: ML problems start with data for which you already know the target answer - what we call **"labeled data"**. In supervised ML, the algorithm teaches itself to learn from the labeled examples that we provide. 

Each example/observation in your data must contain two elements:
- The answer that you want to predict. You provide data that is labeled with the correct answer to the ML algorithm to learn from. Then, you will use the trained ML model to predict this answer on data for which you do not know the correct answer.
- Variables/features: Attributes that can be used to identify patterns to predict the correct answer. If the classes that you are trying to use ML to distinguish do not differ, you may have problems running inference using your model. 

***Representative data***: When collecting data, be thinking about what you are trying to classify and whether the data truly represents that class. From Edge Impulse's ['Introduction to Embedded Machine Learning'](https://www.coursera.org/learn/introduction-to-embedded-machine-learning/lecture/fARmQ/data-collection): 

<p align="center">
  <img src="https://i.imgur.com/DEsftmt.jpg" width="600"/>
</p>

> For example, let's say we are trying to create a model that classifies pictures of dogs, and we feed it a bunch of pictures of poodles as our training data. We then test the model with an image of a Welsh Corgi. Do you think that the model will be able to classify this as a dog or not? And the answer is, it depends. Maybe the model generalizes enough to pick out common features like eyes, a snout, round nose, and so on. However, what you'll likely find is that the model trained on features unique to what we gave it in the training data. Such as, curly fur, and floppy ears. We'll have a hard time classifying new images that do not have these features. Most modern machine learning algorithms are terrible at generalizing and we must take great care to ensure that the data they learn from represents the data we ultimately want them to work with. 

Note also that your model needs to be exposed to all the things you want it to understand: if your model is trained to distinguish different breeds of dogs and you show it an image of a cat, it will attempt to classify the cat as one of the dog breeds. 

***Balanced data***: In addition to having data from multiple difference classess, the data across classes should be **balanaced**. That means having approximately equal numbers of samples in each class. For example, if you have four classes you want to distinguish, you should have ~25% of samples for each class. Note: the balance of data in your training set may not reflect the distribution of data in the real world! You may not expect to encounter one of your classes very frequently in the real world; however, your classifier will perform poorly on this class if it is trained on fewer samples of this class than of the other classes. 

### 3. Train your model 

Data is randomly divided into a **training set**, a **validation set**, and a **test set**.

During training, the training set and associated labels will be used to update the parameters in the machine learning model as the model figures out how to associate labels and data. 

The model's performance is then evaluated on the validation set, which is data that - until now - it has never seen before. If the model performs poorly on the validation set, we can go back, tweak, and retrain the model. The new model will again be tested on the validation set, and the entire process repeated until weare happy with how the model performs. 

Only now do we run the model on the test set. The performance metrics we get when running the model on the test set tell us how good the model is at accomplishing its task. We keep the test set separate from the validation step because every time we iteratively retain and test the model on the validation data, it learns some of the characteristics of the validation dataset. The test data is a check - the model could perform very well on the validation data but poorly on the test data if it has been "overfit" to the input data, i.e., has memorized the data it has been exposed to and is unable to generalize to unseen examples. 

<p align="center">
  <img src="https://meredithspalmer.weebly.com/uploads/1/1/8/5/118542972/holdoutmethod_orig.png" width="600"/>
</p>

Image credit: Edge Impulse ['Introduction to Embedded Machine Learning'](https://www.coursera.org/learn/introduction-to-embedded-machine-learning/lecture/fARmQ/data-collection)

## What will we do today? 

Today, we'll cover how to use pre-trained models for classifying camera trap images (**MegaDetector**), how to build your own models for processing audio data (**Edge Impulse**), and discuss how high-quality data can make AI more useful and successful in your work. Below is a brief overview of the algorithms/programs we'll be working with: 

### MegaDetector 

MegaDetector is a free, open-source image recognition system designed to detect wildlife, humans, and vehicles in camera trap images. Created by Microsoft and trained on millions of images from afround the world, this deep learning algorithm can be used to help automate the processing of images at far faster rates than would be possible by relying on manual human labor alone by identifying which images do not contain wildlife. 

<p align="center">
  <img src="https://i.imgur.com/e9A9boY.png" width="600"/>
</p>

Image credit: [Sara Beery](https://beerys.github.io/)

### Edge Impuse 

Edge Impulse is a platform for developing machine learning algorithms, specifically those designed to work on "edge" devices (e.g., on sensors, processing data as it is collected in the field). We will use the Edge Impulse interface to train an algorithm which can detect gunshots from passive acoustic monitors - this type of algorithm could then be deployed onto a passive acoustic sensor and be used to alert rangers or park managers to illegal poaching incidents. 

<p align="center">
  <img src="https://i.imgur.com/Bo9GXW9.png" width="600"/>
</p>

Image credit: [Edge Impulse](https://www.edgeimpulse.com/)

## Other opportunities and future directions 

Previously inaccessible areas of ecological and conservation interest can now be studied in intense detail thanks to new break-throughs in remote sensing and conservation technology. In addition to camera traps and acoustic sensors, ML can be employed to help process and analyse data from: 
- On-animal sensors (e.g., GPS trackers, accelerometers, microphones, video cameras, heart-rate monitors)
- Remote sensing footage (e.g., unmanned aerial vehicles (UAVS) or drones, satellites)
- Crowd-sourced data (e.g., data from iPhones, images collected by iNaturalist) 
   - As a note, crowdsourcing classifications through platforms such as the [Zooniverse](https://zooniverse.org) can help produce the [massive labeled image datasets](https://www.nature.com/articles/sdata201526?origin=app) necessary to train AI algorithms 

AI are also becoming more sophisticated, extracting more different kinds of information from Big Data. For instance, camera trap image recognition algorithsm are not only getting better at detecting and classifying species, but now are being trained to extract trickier metrics, such as: 
- Counts (how many animals are there?)
- Behavior (what are they doing?)
- Multiple species (classifying >1 species per image) 
- Individual re-identification (recognizing specific individuals) 
- 3D recovery of animal pose (what is the animal doing) 
- 3D reconstructions of the environment (what is the animal navigating) 

For a more in-depth discussion, check out ["Perspectives in machine learning for wildlife conservation"](https://www.nature.com/articles/s41467-022-27980-y) and ["Applications for deep learning in ecology"](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13256).  

## Definitions: 
-	**Algorithm**: A sequence of instructions to a computer, like a recipe on how to process some information
-	**Application**: An application figures what inputs to feed into the model and determines output (i.e., the code running on device)
- **Artificial Intelligence (AI)**: A discipline concerned with the designing of computers that make predictions and decisions
- **Computer Vision**: A branch of AI that works on computer’s ability to understand visual information like pictures or video
- **Convolutional Neural Network**: A neural network that looks at overlapping sections of an image to identify things about the image
- **Data preprocessing**: The program cannot always work with all the data that comes in. Preprocessing ensures none of the data is out of range or unreadable, or has any other problems for the program.
- **Datasets**: Data have collected about thing want to understand and labelled with 'ground-truthed' classifications 
- **GPU**: Graphics Processing Unit - a computer that is specialized for dealing with Graphics
- **Inference**: Taking input and feeding it through the model to get output with some kind of prediction 
- **Library**: A group of programming functions that might be useful together so other programmers don’t have to re-write them
- **Machine Learning**: A branch of AI where programs build models that they can use to make decisions based on the data given to them by programmers
- **Model**: An artifact you get after feeding data into ML algorithm (feed inputs, get output)
- **Pixel**: The smallest, indivisible unit comprising an image, like a tiny square (really tiny if you have a high-resolution screen).
-	**Metadata**: Information about a picture - where it was taken, when, etc.
-	**Neural Network**: Several connected functions that collectively recognize patterns and answer questions
-	**Processor**: The part of your computer that does the “computing.” The main processor on your computer is called the CPU, but other parts might be used for processing other things.
- **Training**: The process of feeding data into model and having it learn how to discern classes 
-	**Weight**: How important each part of information is in deciding what the whole means

- **True negative**: 
- **False negative**: 
- Recall
- Accuracy


